{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions that are used in the rest of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nodes(df):\n",
    "    \"\"\"\n",
    "    Extracts the nodes from a dataframe\n",
    "    :param df: dataframe with source and target columns\n",
    "    :return: set of nodes\n",
    "    \"\"\"\n",
    "    return set(df['source'].unique()).union(set(df['target'].unique()))\n",
    "\n",
    "def load_nodes(file_path):\n",
    "    \"\"\"\n",
    "    Loads the nodes from a file\n",
    "    :param file_path: path to the file\n",
    "    :return: set of nodes\n",
    "    \"\"\"\n",
    "    # much faster to do the operations on dataframes\n",
    "    df = pd.read_csv(file_path, delimiter = ' ', names=['source', 'target', 'day'])\n",
    "    nodes = extract_nodes(df)\n",
    "    print(f\"loaded {file_path} has {len(nodes)} nodes\")\n",
    "    return nodes\n",
    "\n",
    "def largest_connected_component(G):\n",
    "    \"\"\"\n",
    "    Returns the largest connected component of a graph\n",
    "    :param G: graph\n",
    "    :return: largest connected component\n",
    "    \"\"\"\n",
    "    # not supported for directed networks\n",
    "    subgraphs = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "    return G.subgraph(subgraphs[0])\n",
    "\n",
    "def build_network(df, directed = False):\n",
    "    \"\"\"\n",
    "    Builds a network from a dataframe\n",
    "    :param df: dataframe with source and target columns\n",
    "    :param directed: whether the network is directed or not\n",
    "    :return: networkx graph\n",
    "    \"\"\"\n",
    "    if directed:\n",
    "        G = nx.DiGraph()\n",
    "    else:\n",
    "        G = nx.Graph()\n",
    "\n",
    "    # Check if exists a day column\n",
    "    if \"day\" in df.columns:\n",
    "        to_add = [(source, target, {\"day\": day}) for source, target, day in zip(df[\"source\"], df[\"target\"], df[\"day\"])]\n",
    "    else:\n",
    "        to_add = zip(df[\"source\"], df[\"target\"])\n",
    "    G.add_edges_from(to_add)\n",
    "    return G\n",
    "\n",
    "def extract_bidirectional_subnetwork(G):\n",
    "    \"\"\"\n",
    "    creates an undirected sub network from a directed network\n",
    "\n",
    "    that only contains the edges where both\n",
    "\n",
    "    u -> v and v -> u\n",
    "\n",
    "    are in the directed network. In the followers/following sense,\n",
    "\n",
    "    This extracts the sub network where the users follow eachother.\n",
    "\n",
    "    \"\"\"\n",
    "    sub_G = nx.Graph()\n",
    "\n",
    "    for e in G.edges:\n",
    "        if len(e) == 2:\n",
    "            u, v = e\n",
    "        else:\n",
    "            u, v, _ = e\n",
    "        if G.has_edge(v, u):\n",
    "            sub_G.add_edge(u,v)\n",
    "\n",
    "    return sub_G\n",
    "\n",
    "def load_subnetwork(file_path, sub_nodes, directed = False):\n",
    "    \"\"\"\n",
    "    Loads a network from a file and extracts a subnetwork by only keeping the nodes in sub_nodes\n",
    "    :param file_path: path to the file with the network\n",
    "    :param sub_nodes: set of nodes to keep\n",
    "    :param directed: whether the network is directed or not\n",
    "    :return: networkx graph\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(file_path, delimiter = ' ', names=['source', 'target', 'day'])\n",
    "    if df['day'].isna().all():\n",
    "        df.drop(columns=['day'], inplace=True)\n",
    "    sub_df = df[((df['source'].isin(sub_nodes)) & (df['target'].isin(sub_nodes)))]\n",
    "\n",
    "    return build_network(sub_df, directed)\n",
    "\n",
    "def get_friendships(G):\n",
    "    \"\"\"\n",
    "    Extracts the friendships from a social network defined as the subnetwork where both u -> v and v -> u are in the network\n",
    "    :param G: social network\n",
    "    :return: friendships network\n",
    "    \"\"\"\n",
    "\n",
    "    G = extract_bidirectional_subnetwork(G)\n",
    "\n",
    "    return G.to_undirected()\n",
    "\n",
    "def get_lcc(G, directed = False):\n",
    "    \"\"\"\n",
    "    Returns the largest connected component of a graph\n",
    "    :param G: graph\n",
    "    :param directed: whether the graph is directed or not\n",
    "    :return: largest connected component\n",
    "    \"\"\"\n",
    "    lcc = largest_connected_component(G.to_undirected())\n",
    "\n",
    "    if directed:\n",
    "        return lcc.to_directed()\n",
    "    else:\n",
    "        return lcc.to_undirected()\n",
    "\n",
    "def get_subgraph(G, max_n_of_nodes=None, is_seed_node_most_connected=True, seed = None):\n",
    "    \"\"\"\n",
    "    Returns a subgraph of G with k nodes generated by BFS from a seed node\n",
    "    :param G: graph\n",
    "    :param k: number of nodes in the subgraph\n",
    "    :param is_seed_node_most_connected: whether the seed node is the most connected node or a random node\n",
    "    :param seed: seed node, random if None\n",
    "    :return: subgraph\n",
    "    \"\"\"\n",
    "    if max_n_of_nodes is None:\n",
    "        max_n_of_nodes = G.number_of_nodes() // 5\n",
    "\n",
    "    if seed:\n",
    "        seed_node = seed\n",
    "    elif is_seed_node_most_connected:\n",
    "        seed_node = max(G.degree(), key=lambda x: x[1])[0]\n",
    "    else:\n",
    "        # Choosing a random node does not guarantee getting a subgraph of the desired size\n",
    "        seed_node = random.choice(list(G.nodes()))\n",
    "    # Initialize a queue for BFS and a set for visited nodes\n",
    "    queue = [seed_node]\n",
    "    visited = set([seed_node])\n",
    "    # Initialize the subgraph with the starting node\n",
    "    subgraph = nx.Graph()\n",
    "    subgraph.add_node(seed_node)\n",
    "    # While the subgraph has fewer than n nodes and the queue is not empty\n",
    "    while len(subgraph) < max_n_of_nodes and queue:\n",
    "        # Get the next node from the queue\n",
    "        curr_node = queue.pop(0)\n",
    "        # Add its neighbors that have not been visited to the queue and the subgraph\n",
    "        for neighbor in G.neighbors(curr_node):\n",
    "            if neighbor not in visited:\n",
    "                if len(subgraph) >= max_n_of_nodes:\n",
    "                    break\n",
    "                visited.add(neighbor)\n",
    "                subgraph.add_node(neighbor)\n",
    "                subgraph.add_edge(curr_node, neighbor, day=G[curr_node][neighbor][\"day\"])\n",
    "                queue.append(neighbor)\n",
    "    # print n of nodes in the subgraph \n",
    "    print(f\"subgraph has {len(subgraph)} nodes\")\n",
    "    return subgraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset of replies, mentions and retweets with real timestamps (not just days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_time_path = \"../network-data/higgs-activity_time.txt\"\n",
    "activity_time = pd.read_csv(activity_time_path, delimiter = ' ', names=['source', 'target', 'time', 'type'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create high resolution timestamped datasets\n",
    "reply_timestamps_file_path = \"../output/reply_timestamps.edgelist\"\n",
    "mention_timestamps_file_path = \"../output/mention_timestamps.edgelist\"\n",
    "retweet_timestamps_file_path = \"../output/retweet_timestamps.edgelist\"\n",
    "\n",
    "activity_time[activity_time['type'] == 'RE'][['source', 'target', 'time']]\\\n",
    ".to_csv(reply_timestamps_file_path, sep=' ', header=False, index=False)\n",
    "replies = activity_time[activity_time['type'] == 'RE'].copy()\n",
    "# replies[['source', 'target']] = replies[['target', 'source']]\n",
    "replies[['source', 'target', 'time']].to_csv(reply_timestamps_file_path, sep=' ', header=False, index=False)\n",
    "# activity_time[activity_time['type'] == 'MT'][['source', 'target', 'time']]\\\n",
    "# .to_csv(mention_timestamps_file_path, sep=' ', header=False, index=False)\n",
    "mentions = activity_time[activity_time['type'] == 'MT'].copy()\n",
    "#mentions[['source', 'target']] = mentions[['target', 'source']]\n",
    "mentions[['source', 'target', 'time']].to_csv(retweet_timestamps_file_path, sep=' ', header=False, index=False)\n",
    "# Swap source and target columns in the retweet dataset to effectively simulate the dataflow (as mentioned on the website)\n",
    "retweets = activity_time[activity_time['type'] == 'RT'].copy()\n",
    "retweets[['source', 'target']] = retweets[['target', 'source']]\n",
    "retweets[['source', 'target', 'time']].to_csv(retweet_timestamps_file_path, sep=' ', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_file_path = \"../network-data/higgs-social_network.edgelist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the intersection of nodes across all interaction mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reply_nodes = load_nodes(reply_timestamps_file_path)\n",
    "mention_nodes = load_nodes(mention_timestamps_file_path)\n",
    "reply_mention_nodes = reply_nodes.intersection(mention_nodes)\n",
    "del reply_nodes, mention_nodes\n",
    "\n",
    "\n",
    "retweet_nodes = load_nodes(retweet_timestamps_file_path)\n",
    "reply_mention_retweet_nodes = reply_mention_nodes.intersection(retweet_nodes)\n",
    "del reply_mention_nodes, retweet_nodes\n",
    "\n",
    "print(f\"intersected network has {len(reply_mention_retweet_nodes)} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the social network and the friendship network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following/followers is a directed network\n",
    "social_network = load_subnetwork(social_file_path, reply_mention_retweet_nodes, directed=True)\n",
    "# Print information about the directionality of the network\n",
    "lcc_social_network = get_lcc(social_network)\n",
    "friends_network = get_friendships(social_network)\n",
    "print(friends_network)\n",
    "print(social_network)\n",
    "print(lcc_social_network)\n",
    "del social_network\n",
    "lcc_friends_network = get_lcc(friends_network)\n",
    "del friends_network\n",
    "\n",
    "nx.write_edgelist(lcc_friends_network, '../output/higgs-friends-lcc.edgelist')\n",
    "nx.write_edgelist(lcc_social_network, '../output/higgs-social-lcc.edgelist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the subnetworks for replies, mentions and retweets bby only keeping the nodes that are present in the social network or the friendship network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_reply_network = load_subnetwork(reply_timestamps_file_path, lcc_social_network.nodes, directed=True)\n",
    "social_mention_network = load_subnetwork(mention_timestamps_file_path, lcc_social_network.nodes, directed=True)\n",
    "social_retweet_network = load_subnetwork(retweet_timestamps_file_path, lcc_social_network.nodes, directed=True)\n",
    "print(social_reply_network)\n",
    "print(social_mention_network)\n",
    "print(social_retweet_network)\n",
    "friends_reply_network = load_subnetwork(reply_timestamps_file_path, lcc_friends_network.nodes, directed=True)\n",
    "friends_mention_network = load_subnetwork(mention_timestamps_file_path, lcc_friends_network.nodes, directed=True)\n",
    "friends_retweet_network = load_subnetwork(retweet_timestamps_file_path, lcc_friends_network.nodes, directed=True)\n",
    "print(friends_reply_network)\n",
    "print(friends_mention_network)\n",
    "print(friends_retweet_network)\n",
    "paths = ['../output/higgs-social-reply.edgelist', '../output/higgs-social-mention.edgelist', '../output/higgs-social-retweet.edgelist', '../output/higgs-friends-reply.edgelist', '../output/higgs-friends-mention.edgelist', '../output/higgs-friends-retweet.edgelist']\n",
    "networks = [social_reply_network, social_mention_network, social_retweet_network, friends_reply_network, friends_mention_network, friends_retweet_network]\n",
    "\n",
    "\n",
    "for path, network in zip(paths, networks):\n",
    "    nx.write_edgelist(network, path)\n",
    "    with open(path, 'r') as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    # Remove the \"day\" key from each line of data\n",
    "    data = [line.strip().replace(\"{'day': \", \"\").replace(\"}\", \"\") for line in data]\n",
    "\n",
    "    # Convert the data into a list of tuples with integer values\n",
    "    data = [tuple(map(int, line.split())) for line in data]\n",
    "\n",
    "    # Write the modified data to a new file\n",
    "    with open(path, 'w') as f:\n",
    "        for line in data:\n",
    "            f.write(f\"{line[0]} {line[1]} {line[2]}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_file_path = \"../output/higgs-friends-lcc.edgelist\"\n",
    "friends_reply_path = \"../output/higgs-friends-reply.edgelist\"\n",
    "friends_mention_path = \"../output/higgs-friends-mention.edgelist\"\n",
    "friends_retweet_path = \"../output/higgs-friends-retweet.edgelist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal analysis of the reply network (Can be skipped now that we have a better way to do it presented in the next section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"G_reply = nx.read_edgelist(friends_reply_path, data=((\"day\", int),), create_using=nx.DiGraph)\n",
    "subgraph_reply = get_subgraph(G_reply, 5000, is_seed_node_most_connected=True)\n",
    "print(subgraph_reply.number_of_nodes())\n",
    "subgraph_reply_df = pd.DataFrame(subgraph_reply.edges(data=True), columns=['source', 'target', 'attribute'])\n",
    "# Extract the 'day' attribute from the 'attribute' column and add it as a separate column\n",
    "subgraph_reply_df['timestamp'] = [d['day'] for d in subgraph_reply_df['attribute']]\n",
    "min_t = min(subgraph_reply_df['timestamp'])\n",
    "\n",
    "# Hardcoded normalizing to ~1000 timesteps\n",
    "subgraph_reply_df['day'] = ((subgraph_reply_df['timestamp'] - min_t)/ 600).astype(int)\n",
    "# subgraph_reply_df['day'] = ((subgraph_reply_df['timestamp'] - min_t)).astype(int)\n",
    "# Drop the 'attribute' column, which is no longer needed\n",
    "subgraph_reply_df.drop('attribute', axis=1, inplace=True)\n",
    "\n",
    "# there are days above 7, so we need to remove them\n",
    "# subgraph_reply_df = subgraph_reply_df[subgraph_reply_df['day'] <= 7]\n",
    "#unique_days = subgraph_reply_df['day'].unique()\n",
    "#print(unique_days)\n",
    "\n",
    "subgraph_reply_df.sort_values('day')\n",
    "# nx.draw(subgraph_reply)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#  First implementation of temporal spreading\n",
    "# From the plot I am not sure if this is correct, the number of infected nodes is pretty low,\n",
    "# but that could be because the huge majority of interactions happen at time 1, and i am considering a node as spreader from the next day is infected\n",
    "# if we remove this condition I expect the number of infected nodes to be almost the same as the number of nodes in the subgraph at day 1\n",
    "\n",
    "subgraph_reply_df = subgraph_reply_df.sort_values(by=['day'])\n",
    "subgraph_reply_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "iterations = 1\n",
    "infected_nodes_time_dict = {0:[1 for _ in range(iterations)]}\n",
    "seed_nodes = []\n",
    "\n",
    "min_day = min(subgraph_reply_df['day'])\n",
    "max_day = max(subgraph_reply_df['day'])\n",
    "number_of_nodes = len(subgraph_reply.nodes())\n",
    "for n in range(iterations):\n",
    "    seed_node = random.choice(list(subgraph_reply.nodes()))\n",
    "    seed_nodes.append(seed_node)\n",
    "    infected_nodes = {seed_node}\n",
    "\n",
    "    cur_timestamp = min_day\n",
    "    nodes_infected_in_timestamp = set()\n",
    "\n",
    "    # Iterate over all days\n",
    "    for day in subgraph_reply_df['day']:\n",
    "        print(f'Iteration {n+1}/{iterations} : {day}/{max_day} --- ', end='\\r')\n",
    "        # For each day get the edges that were created on that day\n",
    "        edges = subgraph_reply_df[subgraph_reply_df['day'] == day]\n",
    "        # Take the dataframe subset where either the source or the target is in the infected nodes\n",
    "        susceptible_nodes_rows = edges[edges['source'].isin(infected_nodes) | edges['target'].isin(infected_nodes)]\n",
    "        # Make a set of all the nodes that are in susceptible_nodes_rows\n",
    "        susceptible_nodes = set(susceptible_nodes_rows['source']).union(set(susceptible_nodes_rows['target']))\n",
    "        # Add the susceptible nodes to the infected nodes\n",
    "        infected_nodes = infected_nodes.union(susceptible_nodes)\n",
    "        # Add the infected nodes to the infected_nodes_time_dict\n",
    "        if day in infected_nodes_time_dict:\n",
    "            infected_nodes_time_dict[day].append(len(infected_nodes))\n",
    "        else:\n",
    "            infected_nodes_time_dict[day] = [len(infected_nodes)]\n",
    "\n",
    "    # Take the average of the infected nodes for each timestamp\n",
    "    infected_nodes_time_dict = {k: [np.sum(v)/(len(v)*number_of_nodes)] for k, v in infected_nodes_time_dict.items()}\n",
    "    plt.clf()\n",
    "    plt.plot(list(infected_nodes_time_dict.keys()), list(infected_nodes_time_dict.values()))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to find correlations between different networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_reply = nx.read_edgelist(friends_reply_path, data=((\"day\", int),), create_using=nx.DiGraph)\n",
    "G_mention = nx.read_edgelist(friends_mention_path, data=((\"day\", int),), create_using=nx.DiGraph)\n",
    "G_retweet = nx.read_edgelist(friends_retweet_path, data=((\"day\", int),), create_using=nx.DiGraph)\n",
    "common_nodes = set(G_reply.nodes()).intersection(set(G_mention.nodes())).intersection(set(G_retweet.nodes()))\n",
    "\n",
    "print(common_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave only the rows where the source and the target are unique pairs\n",
    "# subgraph_reply_unique_df = subgraph_reply_df.drop_duplicates(subset=['source', 'target'])\n",
    "# degrees = subgraph_reply_unique_df.groupby('source').count()\n",
    "reply_degrees = G_reply.degree()\n",
    "retweet_degrees = G_retweet.degree()\n",
    "mention_degrees = G_mention.degree()\n",
    "\n",
    "# Create a numpy array from reply_degrees where the index is the node id and the value is the degree\n",
    "i = 0\n",
    "node_index_dict = {}\n",
    "for k, v in reply_degrees:\n",
    "    node_index_dict[k] = i\n",
    "    i += 1\n",
    "for k,v in retweet_degrees:\n",
    "    if k not in node_index_dict.keys():\n",
    "        node_index_dict[k] = i\n",
    "        i += 1\n",
    "for k,v in mention_degrees:\n",
    "    if k not in node_index_dict.keys():\n",
    "        node_index_dict[k] = i\n",
    "        i += 1\n",
    "\n",
    "total_length = len(reply_degrees) + len(retweet_degrees) + len(mention_degrees)\n",
    "reply_degrees_array = np.zeros(total_length)\n",
    "for k, v in reply_degrees:\n",
    "    reply_degrees_array[node_index_dict[k]] = v\n",
    "retweet_degrees_array = np.zeros(total_length)\n",
    "for k, v in retweet_degrees:\n",
    "    retweet_degrees_array[node_index_dict[k]] = v\n",
    "mention_degrees_array = np.zeros(total_length)\n",
    "for k, v in mention_degrees:\n",
    "    mention_degrees_array[node_index_dict[k]] = v\n",
    "\n",
    "# Remove trailing zeros from the arrays\n",
    "# reply_degrees_array = reply_degrees_array[~np.all(reply_degrees_array == 0)]\n",
    "# retweet_degrees_array = retweet_degrees_array[~np.all(retweet_degrees_array == 0)]\n",
    "# mention_degrees_array = mention_degrees_array[~np.all(mention_degrees_array == 0)]\n",
    "\n",
    "# Calculate the correlation between the degrees of the different networks\n",
    "print(f\"Correlation between reply and retweet degrees: {np.corrcoef(reply_degrees_array, retweet_degrees_array)[0,1]}\")\n",
    "print(f\"Correlation between reply and mention degrees: {np.corrcoef(reply_degrees_array, mention_degrees_array)[0,1]}\")\n",
    "print(f\"Correlation between retweet and mention degrees: {np.corrcoef(retweet_degrees_array, mention_degrees_array)[0,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate autocorrelation of the number of infected nodes when starting from the node with maximum average degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the nodes that have non-zero degree in all the networks\n",
    "common_nodes = set(G_reply.nodes()).intersection(set(G_mention.nodes())).intersection(set(G_retweet.nodes()))\n",
    "# Construct a subgraph of each of the reply/mention/retweet graphs with only the common nodes but edges between nodes that are not in the common nodes\n",
    "print(G_reply.number_of_nodes())\n",
    "print(G_mention.number_of_nodes())\n",
    "print(G_retweet.number_of_nodes())\n",
    "print(len(common_nodes))\n",
    "G_common_reply = G_reply.subgraph(common_nodes)\n",
    "G_common_mention = G_mention.subgraph(common_nodes)\n",
    "G_common_retweet = G_retweet.subgraph(common_nodes)\n",
    "print(G_common_reply.number_of_nodes())\n",
    "print(G_common_mention.number_of_nodes())\n",
    "print(G_common_retweet.number_of_nodes())\n",
    "# Create degrees_array with the degrees of the nodes in the common nodes subgraph\n",
    "degrees_array = np.zeros(total_length)\n",
    "# finding the average max degree in this way, has the risk of having a node with very low degree in one of the networks and very high degree in the other two, as it is the case\n",
    "for node in G_common_reply.nodes():\n",
    "    degrees_array[node_index_dict[node]] = np.sqrt(G_common_reply.out_degree(node)) + np.sqrt(G_common_mention.out_degree(node)) + np.sqrt(G_common_retweet.out_degree(node))\n",
    "# Get the index of the node with the maximum average degree\n",
    "# print(\"Degrees\", degrees_array)\n",
    "# Get index of the maximum degree\n",
    "max_degree_index = np.argmax(degrees_array)\n",
    "# Get the node with the maximum average degree\n",
    "max_degree_node = list(node_index_dict.keys())[list(node_index_dict.values()).index(max_degree_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start spreading from the node with the maximal out_degree in each of the networks\n",
    "max_degree_reply = max(G_reply.out_degree(), key=lambda x: x[1])[0]\n",
    "max_degree_retweet = max(G_retweet.out_degree(), key=lambda x: x[1])[0]\n",
    "max_degree_mention = max(G_mention.out_degree(), key=lambda x: x[1])[0]\n",
    "print(G_reply.out_degree(max_degree_reply))\n",
    "print(G_retweet.out_degree(max_degree_retweet))\n",
    "print(G_mention.out_degree(max_degree_mention))\n",
    "# Get the subgraph of the node with the maximum average degree\n",
    "subgraph_reply = get_subgraph(G_reply, max_n_of_nodes=5000, seed=max_degree_reply)\n",
    "subgraph_mention = get_subgraph(G_mention, max_n_of_nodes=5000, seed=max_degree_mention)\n",
    "subgraph_retweet = get_subgraph(G_retweet, max_n_of_nodes=5000, seed=max_degree_retweet)\n",
    "print(subgraph_retweet)\n",
    "print(subgraph_mention)\n",
    "print(subgraph_reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for simulating spreading\n",
    "def simulate_spreading(graph_df, seed_nodes, start_day, end_day, name=\"name_not_given\", plot=True):\n",
    "\n",
    "    infected_nodes_ratio_time_count = []\n",
    "    infected_nodes_new = []\n",
    "    \n",
    "    for i, seed_node in enumerate(seed_nodes):\n",
    "        \n",
    "        infected_nodes_time = []\n",
    "        infected_nodes = {seed_node}\n",
    "\n",
    "        min_day = start_day\n",
    "        max_day = end_day\n",
    "        number_of_nodes = len(subgraph_reply.nodes())\n",
    "\n",
    "        # Iterate over all days\n",
    "        for day in np.arange(min_day, max_day+1):\n",
    "            print(f'Graph: {name} Node: {i+1}/{len(seed_nodes)} Timestamp: {day}/{max_day} --- ', end='\\r')\n",
    "            # For each day get the edges that were created on that day\n",
    "            edges = graph_df[graph_df['day'] == day]\n",
    "            # Take the dataframe subset where either the source or the target is in the infected nodes\n",
    "            susceptible_nodes_rows = edges[edges['source'].isin(infected_nodes) | edges['target'].isin(infected_nodes)]\n",
    "            # Make a set of all the nodes that are in susceptible_nodes_rows\n",
    "            susceptible_nodes = set(susceptible_nodes_rows['source']).union(set(susceptible_nodes_rows['target']))\n",
    "            # Add the susceptible nodes to the infected nodes\n",
    "            infected_nodes = infected_nodes.union(susceptible_nodes)\n",
    "            # Add the infected nodes to the infected_nodes_time_dict\n",
    "            infected_nodes_time.append(infected_nodes.copy())\n",
    "\n",
    "        # Take the number of infected nodes per timestamp\n",
    "        infected_nodes_ratio_time_count_it = [len(infected_nodes)/number_of_nodes for infected_nodes in infected_nodes_time]\n",
    "        # Add legend to the plot\n",
    "\n",
    "        last_infected_n = 0\n",
    "        infected_nodes_new_it = []\n",
    "\n",
    "        # add the difference between current infected and last infected\n",
    "        for inf_nodes in infected_nodes_time:\n",
    "            n_inf = len(inf_nodes) - last_infected_n\n",
    "            infected_nodes_new_it.append(n_inf/number_of_nodes)\n",
    "            last_infected_n = len(inf_nodes)\n",
    "            \n",
    "        infected_nodes_ratio_time_count.append(infected_nodes_ratio_time_count_it)\n",
    "        infected_nodes_new.append(infected_nodes_new_it)\n",
    "    \n",
    "    # average over iterations\n",
    "    infected_nodes_ratio_time_count = np.mean(infected_nodes_ratio_time_count, axis=0)\n",
    "    infected_nodes_new = np.mean(infected_nodes_new, axis=0)\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(np.arange(min_day, max_day+1), infected_nodes_new, label=f'{name}', alpha=0.8)\n",
    "        plt.legend()\n",
    "    return infected_nodes_ratio_time_count, infected_nodes_new\n",
    "\n",
    "def build_df_from_graph(graph):\n",
    "    graph_df = pd.DataFrame(graph.edges(data=True), columns=['source', 'target', 'attribute'])\n",
    "    # Extract the 'day' attribute from the 'attribute' column and add it as a separate column\n",
    "    graph_df['timestamp'] = [d['day'] for d in graph_df['attribute']]\n",
    "    min_t = min(graph_df['timestamp'])\n",
    "\n",
    "    # Hardcoded normalizing to ~1000 timesteps\n",
    "    graph_df['day'] = ((graph_df['timestamp'] - min_t) / 600).astype(int)\n",
    "    # subgraph_df['day'] = ((subgraph_df['timestamp'] - min_t)).astype(int)\n",
    "\n",
    "    # Drop the 'attribute' column, which is no longer needed\n",
    "    graph_df.drop('attribute', axis=1, inplace=True)\n",
    "\n",
    "    graph_df.sort_values('day')\n",
    "\n",
    "    # Simulate temporal spreading starting from the node with the highest average degree\n",
    "    graph_df = graph_df.sort_values(by=['day'])\n",
    "    graph_df.reset_index(drop=True, inplace=True)\n",
    "    return graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select random nodes\n",
    "random_node = np.random.choice(G_common_reply.nodes(), 10)\n",
    "# random_node = '1988'\n",
    "print(random_node, [degrees_array[node_index_dict[node]] for node in random_node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate spreading for the node with the maximum average degree\n",
    "print(subgraph_reply)\n",
    "# Build dataframes for each graph\n",
    "subgraph_reply_df = build_df_from_graph(subgraph_reply)\n",
    "subgraph_mention_df = build_df_from_graph(subgraph_mention)\n",
    "subgraph_retweet_df = build_df_from_graph(subgraph_retweet)\n",
    "\n",
    "start_day_reply = min(subgraph_reply_df['day'])\n",
    "start_day_mention = min(subgraph_mention_df['day'])\n",
    "start_day_retweet = min(subgraph_retweet_df['day'])\n",
    "end_day_reply = max(subgraph_reply_df['day'])\n",
    "end_day_mention = max(subgraph_mention_df['day'])\n",
    "end_day_retweet = max(subgraph_retweet_df['day'])\n",
    "\n",
    "start_day = min(start_day_reply, start_day_mention, start_day_retweet)\n",
    "end_day = max(end_day_reply, end_day_mention, end_day_retweet)\n",
    "start_node = random_node\n",
    "\n",
    "reply_infection_rates, reply_new_infected = simulate_spreading(subgraph_reply_df, start_node, start_day, end_day, name=\"reply\")\n",
    "mention_infection_rates, mention_new_infected = simulate_spreading(subgraph_mention_df, start_node, start_day, end_day, name=\"mention\")\n",
    "retweet_infection_rates, retweet_new_infected = simulate_spreading(subgraph_retweet_df, start_node, start_day, end_day, name=\"retweet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_reply_mention = np.correlate(reply_new_infected, mention_new_infected, \"full\")\n",
    "correlation_reply_retweet = np.correlate(reply_new_infected, retweet_new_infected, \"full\")\n",
    "correlation_mention_retweet = np.correlate(mention_new_infected, retweet_new_infected, \"full\")\n",
    "print(correlation_reply_mention)\n",
    "\n",
    "correlation_reply_mention /= np.max(np.abs(correlation_reply_mention), axis=0)\n",
    "correlation_reply_retweet /= np.max(np.abs(correlation_reply_retweet), axis=0)\n",
    "correlation_mention_retweet /= np.max(np.abs(correlation_mention_retweet), axis=0)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(correlation_reply_mention, label=\"reply-mention\", alpha=0.8)\n",
    "plt.plot(correlation_reply_retweet, label=\"reply-retweet\", alpha=0.8)\n",
    "plt.plot(correlation_mention_retweet, label=\"mention-retweet\", alpha=0.8)\n",
    "plt.legend()\n",
    "plt.title(\"Correlation between infection rates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "correlation_reply_mention = np.corrcoef(reply_infection_rates, mention_infection_rates)[0][1]\n",
    "corr_coefs = []\n",
    "for i in range(len(reply_infection_rates)):\n",
    "    corr_coefs.append(np.corrcoef(reply_infection_rates[:-i], mention_infection_rates[i:])[0][1])\n",
    "correlation_reply_retweet = np.corrcoef(reply_infection_rates, retweet_infection_rates)[0][1]\n",
    "correlation_mention_retweet = np.corrcoef(mention_infection_rates, retweet_infection_rates)[0][1]\n",
    "print(correlation_reply_mention)\n",
    "plt.clf()\n",
    "plt.plot(corr_coefs, label=\"reply-mention\")\n",
    "plt.plot(correlation_reply_retweet, label=\"reply-retweet\")\n",
    "plt.plot(correlation_mention_retweet, label=\"mention-retweet\")\n",
    "plt.legend()\n",
    "plt.title(\"Correlation between infection rates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
